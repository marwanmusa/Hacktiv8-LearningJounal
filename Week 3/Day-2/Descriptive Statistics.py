# -*- coding: utf-8 -*-
"""d2am.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ardhiraka/FSDS_Guidelines/blob/master/p0/w3/d2am.ipynb

# Week 3: Day 2 // Practical Statistics : Descriptive

Statistics is primarily about analyzing data samples, and that starts with understanding the distribution of data in a sample.

# A. Analyzing Data Distribution
A great deal of statistical analysis is based on the way that data values are distributed within the dataset. In this section, we'll explore some statistics that you can use to tell you about the values in a dataset.

---

## A.1. Measures of Central Tendency
The term *measures of central tendency* sounds a bit grand, but really it's just a fancy way of saying that we're interested in knowing **where the middle value in our data** is. For example, suppose decide to conduct a study into the comparative salaries of people who graduated from the same school. You might record the results like this:

| Name     | Salary      |
|----------|-------------|
| Dan      | 50,000      |
| Joann    | 54,000      |
| Pedro    | 50,000      |
| Rosie    | 189,000     |
| Ethan    | 55,000      |
| Vicky    | 40,000      |
| Frederic | 59,000      |

Now, some of the former-students may earn a lot, and others may earn less; but what's the salary in the middle of the range of all salaries?

---

### Mean
A common way to define the central value is to use the *mean*, often called the *average*. **This is calculated as the sum of the values in the dataset, divided by the number of observations in the dataset.** When the dataset consists of the full population, the mean is represented by the Greek symbol ***&mu;*** (*mu*), and the formula is written like this:

$$
\begin{equation}\mu = \frac{\displaystyle\sum_{i=1}^{N}X_{i}}{N}\end{equation}
$$

More commonly, when working with a sample, the mean is represented by ***x&#772;*** (*x-bar*), and the formula is written like this (note the lower case letters used to indicate values from a sample):

$$
\begin{equation}\bar{x} = \frac{\displaystyle\sum_{i=1}^{n}x_{i}}{n}\end{equation}
$$

In the case of our list of heights, this can be calculated as:

$$
\begin{equation}\bar{x} = \frac{50000+54000+50000+189000+55000+40000+59000}{7}\end{equation}
$$

Which is **71,000**.

>In technical terminology, ***x&#772;*** is a *statistic* (an estimate based on a sample of data) and ***&mu;*** is a *parameter* (a true value based on the entire population). A lot of the time, the parameters for the full population will be impossible (or at the very least, impractical) to measure; so we use statistics obtained from a representative sample to approximate them. In this case, we can use the sample mean of salary for our selection of surveyed students to try to estimate the actual average salary of all students who graduate from our school.

In Python, when working with data in a `pandas.dataframe`, you can use the `mean` function, like this:
"""

# Get the Mean

import pandas as pd


df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000]})

print (df['Salary'].mean())

"""### Median
OK, let's see if we can find another definition for the central value that more closely reflects the expected earning potential of students attending our school. Another measure of central tendancy we can use is the *median*. **To calculate the median, we need to sort the values into ascending order and then find the middle-most value.** When there are an odd number of observations, you can find the position of the median value using this formula (where *n* is the number of observations):

$$
\begin{equation}\frac{n+1}{2}\end{equation}
$$

Remember that this formula returns the *position* of the median value in the sorted list; not the value itself.

If the number of observations is even, then things are a little (but not much) more complicated. In this case you calculate the median as the average of the two middle-most values, which are found like this:

$$
\begin{equation}\frac{n}{2} \;\;\;\;and \;\;\;\; \frac{n}{2} + 1\end{equation}
$$

So, for our graduate salaries; first lets sort the dataset:

| Salary      |
|-------------|
| 40,000      |
| 50,000      |
| 50,000      |
| 54,000      |
| 55,000      |
| 59,000      |
| 189,000     |

There's an odd number of observation (7), so the median value is at position (7 + 1) &div; 2; in other words, position 4:

| Salary      |
|-------------|
| 40,000      |
| 50,000      |
| 50,000      |
|***>54,000*** |
| 55,000      |
| 59,000      |
| 189,000     |

So the median salary is **54,000**.

The `pandas.dataframe` class in Python has a `median` function to find the median :
"""

# Get the Median

import pandas as pd


df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000]})

print (df['Salary'].median())

"""### Mode

Another related statistic is the *mode*, **which indicates the most frequently occurring value**. If you think about it, this is potentially a good indicator of how much a student **might expect** to earn when they graduate from the school; out of all the salaries that are being earned by former students, the mode is earned by more than any other.

Looking at our list of salaries, there are two instances of former students earning **50,000**, but only one instance each for all other salaries:

| Salary      |
|-------------|
| 40,000      |
|***>50,000***|
|***>50,000***|
| 54,000      |
| 55,000      |
| 59,000      |
| 189,000     |

The mode is therefore **50,000**.

As you might expect, the `pandas.dataframe` class has a `mode` function to return the mode:
"""

# Get the Mode
import pandas as pd


df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})

print (df['Salary'].mode())

"""##### Multimodal Data
It's not uncommon for a set of data to have more than one value as the mode. For example, suppose Ethan receives a raise that takes his salary to **59,000**:

| Salary      |
|-------------|
| 40,000      |
|***>50,000***|
|***>50,000***|
| 54,000      |
|***>59,000***|
|***>59,000***|
| 189,000     |

Now there are two values with the highest frequency. This dataset is *bimodal*. More generally, when there is more than one mode value, the data is considered *multimodal*.

The `pandas.dataframe.mode` function returns all of the modes:
"""

import pandas as pd


df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,59000,40000,59000]})

print (df['Salary'].mode())

"""---

## A.2. Distribution and Density
Now we know something about finding the center, we can start to explore how the data is distributed around it. What we're interested in here is understanding the general "shape" of the data distribution so that we can begin to get a feel for what a 'typical' value might be expected to be.

We can start by finding the extremes - the minimum and maximum. In the case of our salary data, the lowest paid graduate from our school is Vicky, with a salary of **40,000**; and the highest-paid graduate is Rosie, with **189,000**.

The `pandas.dataframe` class has `min` and `max` functions to return these values.

Run the following code to compare the minimum and maximum salaries to the central measures we calculated previously :
"""

# Get the Mean, Median, Mode, Min, and Max

import pandas as pd

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})

print ('Min: ' + str(df['Salary'].min()))
print ('Mode: ' + str(df['Salary'].mode()[0]))
print ('Median: ' + str(df['Salary'].median()))
print ('Mean: ' + str(df['Salary'].mean()))
print ('Max: ' + str(df['Salary'].max()))

"""We can examine these values, and get a sense for how the data is distributed - for example, we can see that **the *mean* is closer to the max than the *median*, and that both are closer to the *min* than to the *max***.

---

However, it's generally easier to get a sense of the distribution by visualizing the data. Let's start by creating a histogram of the salaries, highlighting the *mean* and *median* salaries (the *min*, *max* are fairly self-evident, and the *mode* is wherever the highest bar is):
"""

# Commented out IPython magic to ensure Python compatibility.
# Create a Histogram of Salary

# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})

salary = df['Salary']
salary.plot.hist(title='Salary Distribution', color='lightblue', bins=25)  
plt.axvline(salary.mean(), color='magenta', linestyle='dashed', linewidth=2)
plt.axvline(salary.median(), color='green', linestyle='dashed', linewidth=2)
plt.show()

"""Notes : 
* Magenta line : mean of salaries.
* Green line : median of salaries.

"""

# Display Value of Bins

import numpy as np

print('Sorted Salary : ', sorted(list(salary)), '\n')

n, bins = np.histogram(salary, bins=25)
for ii in range(0, len(n)):
  print('Bins Border - ' , ii, ': ', bins[ii], ' - ', bins[ii+1], '\t', ' Frequency : ', n[ii])

"""The <span style="color:magenta">***mean***</span> and <span style="color:green">***median***</span> are shown as dashed lines. Note the following:
- *Salary* is a continuous data value - **graduates could potentially earn any value along the scale**, even down to a fraction of cent.
- The number of bins in the histogram determines the size of each salary band for which we're counting frequencies. **Fewer bins means merging more individual salaries together to be counted as a group**.
- **The majority of the data is on the left side of the histogram**, reflecting the fact that most graduates earn between 40,000 and 55,000
- **The mean is a higher value than the median and mode**.
- **There are gaps in the histogram** for salary bands that nobody earns.

The histogram shows the relative frequency of each salary band, based on the number of bins. It also gives us a sense of the *density* of the data for each point on the salary scale. With enough data points, and small enough bins, we could view this density as a line that shows the shape of the data distribution.

---

### Skewness and Kurtosis

Run the following cell to show the density of the salary data as a line on top of the histogram :
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})

salary = df['Salary']
density = stats.gaussian_kde(salary)
n, x, _ = plt.hist(salary, histtype='step', bins=25)  
plt.plot(x, density(x)*10**5)
plt.axvline(salary.mean(), color='magenta', linestyle='dashed', linewidth=2)
plt.axvline(salary.median(), color='green', linestyle='dashed', linewidth=2)
plt.show()

"""Note that the density line takes the form of an asymmetric curve that has **a "peak" on the left and a long tail on the right**. We describe this sort of data distribution as being **skewed**; that is, the data is not distributed symmetrically but "bunched together" on one side. In this case, the data is bunched together on the left, creating a long tail on the right; and is described as being *right-skewed* because some infrequently occurring high values are pulling the *mean* to the right.

---

Let's take a look at another set of data. We know how much money our graduates make, but how many hours per week do they need to work to earn their salaries? Here's the data:

| Name     | Hours |
|----------|-------|
| Dan      | 41    |
| Joann    | 40    |
| Pedro    | 36    |
| Rosie    | 30    |
| Ethan    | 35    |
| Vicky    | 39    |
| Frederic | 40    |

Run the following code to show the distribution of the hours worked:
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Hours':[41,40,36,30,35,39,40]})

hours = df['Hours']
density = stats.gaussian_kde(hours)
n, x, _ = plt.hist(hours, histtype='step', bins=25)  
plt.plot(x, density(x)*7)
plt.axvline(hours.mean(), color='magenta', linestyle='dashed', linewidth=2)
plt.axvline(hours.median(), color='green', linestyle='dashed', linewidth=2)
plt.show()

# Display Value of Bins
import numpy as np

n, bins = np.histogram(hours, bins=25)
for ii in range(0, len(n)):
  print('Bins Border ', ii, ': ', bins[ii], ' - ', bins[ii+1], '\t', ' Frequency : ', n[ii])

"""Once again, the distribution is skewed, but this time it's **left-skewed**. Note that the curve is asymmetric with the <span style="color:magenta">***mean***</span> to the left of the <span style="color:green">***median***</span> and the *mode*; and the average weekly working hours skewed to the lower end.

---

Once again, Rosie seems to be getting the better of the deal. She earns more than her former classmates for working fewer hours. Maybe a look at the test scores the students achieved on their final grade at school might help explain her success:

| Name     | Grade |
|----------|-------|
| Dan      | 50    |
| Joann    | 50    |
| Pedro    | 46    |
| Rosie    | 95    |
| Ethan    | 50    |
| Vicky    | 5     |
| Frederic | 57    |

Let's take a look at the distribution of these grades:
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Grade':[50,50,46,95,50,5,57]})

grade = df['Grade']
density = stats.gaussian_kde(grade)
n, x, _ = plt.hist(grade, histtype='step', bins=25)  
plt.plot(x, density(x)*2**7.5)
plt.axvline(grade.mean(), color='magenta', linestyle='dashed', linewidth=2)
plt.axvline(grade.median(), color='green', linestyle='dashed', linewidth=2)
plt.show()

"""This time, the distribution is symmetric, forming a "bell-shaped" curve. The <span style="color:magenta">***mean***</span>, <span style="color:green">***median***</span>, **and mode are at the same location, and the data tails off evenly on both sides from a central peak.** Statisticians call this a *normal* distribution (or sometimes a *Gaussian* distribution).

---

### Measure of Skewness and Kurtosis
You can measure *skewness* (in which direction the data is skewed and to what degree) and kurtosis (how "peaked" the data is) to get an idea of the shape of the data distribution. In Python, you can use the ***skew*** and ***kurt*** functions to find this:
"""

# Commented out IPython magic to ensure Python compatibility.
# Get Skewness and Kurtosis Value

# %matplotlib inline
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import scipy.stats as stats

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,30,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

numcols = ['Salary', 'Hours', 'Grade']
for col in numcols:
    print(df[col].name + ' skewness: ' + str(df[col].skew()))
    print(df[col].name + ' kurtosis: ' + str(df[col].kurt()))
    density = stats.gaussian_kde(df[col])
    n, x, _ = plt.hist(df[col], histtype='step', bins=25)  
    plt.plot(x, density(x)*6)
    plt.show()
    print('\n')

"""### Measures of Variance
We can see from the distribution plots of our data that the values in our dataset can vary quite widely. We can use various measures to quantify this variance.

---

#### Range
A simple way to quantify the variance in a dataset is to identify the **difference between the lowest and highest values**. This is called the *range*, and is calculated by **subtracting the minimim value from the maximum value**.

The following Python code creates a single Pandas dataframe for our school graduate data, and calculates the *range* for each of the numeric features:
"""

import pandas as pd

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,30,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

numcols = ['Salary', 'Hours', 'Grade']
for col in numcols:
    print(df[col].name + ' range: ' + str(df[col].max() - df[col].min()))

"""#### Percentiles and Quartiles
The range is easy to calculate, but it's not a particularly useful statistic. For example, a range of 149,000 between the lowest and highest salary does not tell us which value within that range a graduate is most likely to earn -  it doesn't tell us nothing about how the salaries are distributed around the mean within that range.  The range tells us very little about the comparative position of an individual value within the distribution - for example, Frederic scored 57 in his final grade at school; which is a pretty good score (it's more than all but one of his classmates); but this isn't immediately apparent from a score of 57 and range of 90.

---

##### Percentiles
A percentile tells us where a given value is ranked in the overall distribution. For example, 25% of the data in a distribution has a value lower than the 25th percentile; 75% of the data has a value lower than the 75th percentile, and so on. Note that half of the data has a value lower than the 50th percentile - so the 50th percentile is also the median!

**Let's examine Frederic's grade using this approach. We know he scored 57, but how does he rank compared to his fellow students?**

Well, there are seven students in total, and five of them scored less than Frederic; so we can calculate the percentile for Frederic's grade like this:

$$
\begin{equation}\frac{5}{7} \times 100 \approx 71.4\end{equation} 
$$

So Frederic's score puts him at the 71.4th percentile in his class.

In Python, you can use the `percentileofscore` function in the `scipy.stats` package to calculate the percentile for a given value in a set of values:
"""

# Get the Percentile (kind='strict')

import pandas as pd
from scipy import stats

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,30,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

print(stats.percentileofscore(df['Grade'], 57, 'strict'))

# Display Data
print(df)
print('')
print(df.sort_values(by=['Grade']).reset_index())

"""---
We've used the strict definition of percentile; but sometimes it's calculated as being the percentage of values that are less than *or equal to* the value you're comparing. In this case, the calculation for Frederic's percentile would include his own score:

$$
\begin{equation}\frac{6}{7} \times 100 \approx 85.7\end{equation} 
$$

You can calculate this way in Python by using the `weak` mode of the `percentileofscore` function:
"""

import pandas as pd
from scipy import stats

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,30,35,39,40],
                   'Price (Millions Dollar)':[50,50,46,95,50,5,57]})

print(stats.percentileofscore(df['Price (Millions Dollar)'], 57, 'weak'))

"""##### Quartiles

Rather than using individual percentiles to compare data, we can consider the overall spread of the data by dividing those percentiles into four *quartiles*. The first quartile contains the values from the minimum to the 25th percentile, the second from the 25th percentile to the 50th percentile (which is the median), the third from the 50th percentile to the 75th percentile, and the fourth from the 75th percentile to the maximum.

In Python, you can use the `quantile` function of the `pandas.dataframe` class to find the threshold values at the 25th, 50th, and 75th percentiles (*quantile* is a generic term for a ranked position, such as a percentile or quartile).

Run the following code to find the quartile thresholds for the weekly hours worked by our former students :
"""

# Quartiles
import pandas as pd

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,17,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})
print(df['Hours'].quantile([0.25, 0.5, 0.75]))

"""---
##### Box Plot

Its usually easier to understand how data is distributed across the quartiles by visualizing it. You can use a histogram, but many data scientists use a kind of visualization called a *box plot* (or a *box and whiskers* plot).

Let's create a box plot for the weekly hours:
"""

# Commented out IPython magic to ensure Python compatibility.
# Display Box Plot of Weekly Hours

# %matplotlib inline
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,30,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

# Plot a box-whisker chart
df['Hours'].plot(kind='box', title='Weekly Hours Distribution', figsize=(10,8))
plt.show()

"""The box plot consists of:
- A rectangular *box* that shows where the data between the 25th and 75th percentile (the second and third quartile) lie. This part of the distribution is often referred to as the *interquartile range* - it contains the middle 50 data values.
- *Whiskers* that extend from the box to the bottom of the first quartile and the top of the fourth quartile to show the full range of the data.
- A line in the box that shows that location of the median (the 50th percentile, which is also the threshold between the second and third quartile)

In this case, you can see that the interquartile range is between 35 and 40, with the median nearer the top of that range. The range of the first quartile is from around 30 to 35, and the fourth quartile is from 40 to 41.

##### Outliers
Let's take a look at another box plot - this time showing the distribution of the salaries earned by our former classmates:
"""

# Commented out IPython magic to ensure Python compatibility.
# Display Box Plot of Salary

# %matplotlib inline
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,30,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

# Plot a box-whisker chart
df['Salary'].plot(kind='box', title='Salary Distribution', figsize=(10,8))
plt.show()

"""So what's going on here?

Well, as we've already noticed, Rosie earns significantly more than her former classmates. So much more in fact, that her salary has been identifed as an *outlier*. **An outlier is a value that is so far from the center of the distribution compared to other values that it skews the distribution by affecting the mean.** There are all sorts of reasons that you might have outliers in your data, including data entry errors, failures in sensors or data-generating equipment, or genuinely anomalous values.

So what should we do about it?

This really depends on the data, and what you're trying to use it for. In this case, let's assume we're trying to figure out what's a reasonable expectation of salary for a graduate of our school to earn. Ignoring for the moment that we have an extremly small dataset on which to base our judgement :
  * It looks as if Rosie's salary could be either an error (maybe she mis-typed it in the form used to collect data) or 
  * A genuine anomaly (maybe she became a professional athelete or some other extremely highly paid job). 

Either way, it doesn't seem to represent a salary that a typical graduate might earn.

Let's see what the distribution of the data looks like without the outlier :
"""

# Commented out IPython magic to ensure Python compatibility.
# Display Box Plot of Salary without Outliers

# %matplotlib inline
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,17,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

# Plot a box-whisker chart
df['Salary'].plot(kind='box', title='Salary Distribution', figsize=(10,8), showfliers=False)
plt.show()

"""Now it looks like there's a more even distribution of salaries. It's still not quite symmetrical, but there's much less overall variance. There's potentially some cause here to disregard Rosie's salary data when we compare the salaries, as it is tending to skew the analysis.

So is that OK? Can we really just ignore a data value we don't like? Again, it depends on what you're analyzing. 

---

Let's take a look at the distribution of final grades:
"""

# Commented out IPython magic to ensure Python compatibility.
# Visualization of Box Plot of Grade

# %matplotlib inline
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,17,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

# Plot a box-whisker chart
df['Grade'].plot(kind='box', title='Grade Distribution', figsize=(10,8))
plt.show()

"""Once again there are outliers, this time at both ends of the distribution. However, think about what this data represents. If we assume that the grade for the final test is based on a score out of 100, **it seems reasonable to expect that some students will score very low (maybe even 0) and some will score very well (maybe even 100)**; but most will get a score somewhere in the middle.  **The reason that the low and high scores here look like outliers might just be because we have so few data points.** Let's see what happens if we include a few more students in our data:"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],
                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})

# Plot a box-whisker chart
df['Grade'].plot(kind='box', title='Grade Distribution', figsize=(10,8))
plt.show()

"""With more data, there are some more high and low scores; so we no longer consider the isolated cases to be outliers.

**The key point to take away here is that you need to really understand the data and what you're trying to do with it, and you need to ensure that you have a reasonable sample size, before determining what to do with outlier values.**

#### Variance and Standard Deviation
We've seen how to understand the *spread* of our data distribution using the range, percentiles, and quartiles; and we've seen the effect of outliers on the distribution. Now it's time to look at how to measure the amount of variance in the data.

---

##### Variance
Variance is measured as the average of the squared difference from the mean. For a full population, it's indicated by a squared Greek letter *sigma* (***&sigma;<sup>2</sup>***) and calculated like this:

$$
\begin{equation}\sigma^{2} = \frac{\displaystyle\sum_{i=1}^{N} (X_{i} -\mu)^{2}}{N}\end{equation}
$$

For a sample, it's indicated as ***s<sup>2</sup>*** calculated like this:

$$
\begin{equation}s^{2} = \frac{\displaystyle\sum_{i=1}^{n} (x_{i} -\bar{x})^{2}}{n-1}\end{equation}
$$

In both cases, we sum the difference between the individual data values and the mean and square the result. Then, for a full population we just divide by the number of data items to get the average. When using a sample, we divide by the total number of items **minus 1** to correct for sample bias.

Let's work this out for our student grades (assuming our data is a sample from the larger student population).

First, we need to calculate the mean grade:

$$
\begin{equation}\bar{x} = \frac{50+50+46+95+50+5+57}{7}\approx 50.43\end{equation}
$$

Then we can plug that into our formula for the variance:

$$
\begin{equation}s^{2} = \frac{(50-50.43)^{2}+(50-50.43)^{2}+(46-50.43)^{2}+(95-50.43)^{2}+(50-50.43)^{2}+(5-50.43)^{2}+(57-50.43)^{2}}{7-1}\end{equation}
$$

So:

$$
\begin{equation}s^{2} = \frac{0.185+0.185+19.625+1986.485+0.185+2063.885+43.165}{6}\end{equation}
$$

Which simplifies to:

$$
\begin{equation}s^{2} = \frac{4113.715}{6}\end{equation}
$$

Giving the result:

$$
\begin{equation}s^{2} \approx 685.619\end{equation}
$$

The higher the variance, the more spread your data is around the mean.

In Python, you can use the ***var*** function of the *pandas.dataframe* class to calculate the variance of a column in a dataframe:
"""

# Get the Variance

import pandas as pd

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,17,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

print(df['Grade'].var())

"""##### Standard Deviation
To calculate the variance, we squared the difference of each value from the mean. If we hadn't done this, the numerator of our fraction would always end up being zero (because the mean is at the center of our values). However, this means that the variance is not in the same unit of measurement as our data - in our case, since we're calculating the variance for grade points, it's in grade points squared; which is not very helpful.

To get the measure of variance back into the same unit of measurement, we need to find its square root:

$$
\begin{equation}s = \sqrt{685.619} \approx 26.184\end{equation}
$$

So what does this value represent?

It's the *standard deviation* for our grades data. More formally, it's calculated like this for a full population:

$$
\begin{equation}\sigma = \sqrt{\frac{\displaystyle\sum_{i=1}^{N} (X_{i} -\mu)^{2}}{N}}\end{equation}
$$

Or like this for a sample:

$$
\begin{equation}s = \sqrt{\frac{\displaystyle\sum_{i=1}^{n} (x_{i} -\bar{x})^{2}}{n-1}}\end{equation}
$$

Note that in both cases, it's just the square root of the corresponding variance forumla!

In Python, you can calculate it using the ***std*** function:
"""

# Get the Standard Deviation

import pandas as pd

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,17,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

print(df['Grade'].std())

"""##### Standard Deviation in a Normal Distribution

In statistics and data science, we spend a lot of time considering *normal* distributions; because they occur so frequently. The standard deviation has an important relationship to play in a normal distribution.

Run the following cell to show a histogram of a *normal* distribution (which is a distribution with a mean of 0 and a standard deviation of 1):
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats

# Create a random standard normal distribution
df = pd.DataFrame(np.random.randn(100000, 1), columns=['Grade'])

# Plot the distribution as a histogram with a density curve
grade = df['Grade']
density = stats.gaussian_kde(grade)
n, x, _ = plt.hist(grade, color='lightgrey', bins=100)  
plt.plot(x, density(x))

# Get the mean and standard deviation
s = df['Grade'].std()
m = df['Grade'].mean()

# Annotate 1 stdev
x1 = [m-s, m+s]
y1 = [0.25, 0.25]
plt.plot(x1,y1, color='magenta')
plt.annotate('1s (68.26%)', (x1[1],y1[1]))

# Annotate 2 stdevs
x2 = [m-(s*2), m+(s*2)]
y2 = [0.05, 0.05]
plt.plot(x2,y2, color='green')
plt.annotate('2s (95.45%)', (x2[1],y2[1]))

# Annotate 3 stdevs
x3 = [m-(s*3), m+(s*3)]
y3 = [0.005, 0.005]
plt.plot(x3,y3, color='orange')
plt.annotate('3s (99.73%)', (x3[1],y3[1]))

# Show the location of the mean
plt.axvline(grade.mean(), color='grey', linestyle='dashed', linewidth=1)

plt.show()

"""<img src='https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module6-RandomError/Normal%20Distribution%20deviations.png'>

The horizontal colored lines show the percentage of data within 1, 2, and 3 standard deviations of the mean (plus or minus).

In any normal distribution:
- Approximately 68.26% of values fall within one standard deviation from the mean.
- Approximately 95.45% of values fall within two standard deviations from the mean.
- Approximately 99.73% of values fall within three standard deviations from the mean.

---

##### Z Score
So in a normal (or close to normal) distribution, **standard deviation provides a way to evaluate how far from a mean** a given range of values falls, allowing us to compare where a particular value lies within the distribution. For example, suppose Rosie tells you she was the highest scoring student among her friends - that doesn't really help us assess how well she scored. She may have scored only a fraction of a point above the second-highest scoring student. Even if we know she was in the top quartile; if we don't know how the rest of the grades are distributed it's still not clear how well she performed compared to her friends.

However, if she tells you how many standard deviations higher than the mean her score was, this will help you compare her score to that of her classmates.

So how do we know how many standard deviations above or below the mean a particular value is? We call this a *Z Score*, and it's calculated like this for a full population:

$$
\begin{equation}Z = \frac{x - \mu}{\sigma}\end{equation}
$$

or like this for a sample:

$$
\begin{equation}Z = \frac{x - \bar{x}}{s}\end{equation}
$$

So, let's examine Rosie's grade of 95. Now that we know the *mean* grade is 50.43 and the *standard deviation* is 26.184, we can calculate the Z Score for this grade like this:

$$
\begin{equation}Z = \frac{95 - 50.43}{26.184} = 1.702\end{equation}.
$$

So Rosie's grade is 1.702 standard deviations above the mean.
"""

# Commented out IPython magic to ensure Python compatibility.
# Visualization of Box Plot of Grade

# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,17,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

# Plot a box-whisker chart
df['Grade'].plot(kind='box', title='Grade Distribution', figsize=(10,8))
plt.show()

# Visualization of Distribution
grade = df['Grade']
density = stats.gaussian_kde(grade)
n, x, _ = plt.hist(grade, histtype='step', bins=25)  
plt.plot(x, density(x)*2**7.5)
plt.axvline(grade.mean(), color='magenta', linestyle='dashed', linewidth=2)
plt.axvline(grade.median(), color='green', linestyle='dashed', linewidth=2)
plt.show()

"""### Summarizing Data Distribution in Python
We've seen how to obtain individual statistics in Python, but you can also use the `describe` function to retrieve summary statistics for all numeric columns in a dataframe. These summary statistics include many of the statistics we've examined so far :
"""

import pandas as pd

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,17,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})
print(df.describe())

"""# B. Data Summarization

Data summarization techniques provide a way to describe the distribution of data using a few key measurements.

The most common example of data summarization is the calculation of the mean and standard deviation for data that has a Gaussian distribution. With these two parameters alone, you can understand and re-create the distribution of the data. The data summary can compress as few as tens or as many as millions individual observations.

The problem is, you cannot easily calculate the mean and standard deviation of data that does not have a Gaussian distribution. Technically, you can calculate these quantities, but they do not summarize the data distribution; in fact, they can be very misleading.

In the case of data that does not have a Gaussian distribution, you can summarize the data sample using the five-number summary.

## B.1 Five-Number Summary

The five-number summary, or 5-number summary for short, is a non-parametric data summarization technique. It is sometimes called the **Tukey 5-number summary** because it was recommended by John Tukey. It can be used to describe the distribution of data samples for data with **any distribution**.

As a standard summary for general use, the 5-number summary provides about the right amount of detail.

The five-number summary involves the calculation of 5 summary statistical quantities: namely:

- **Median** : The middle value in the sample, also called the 50th percentile or the 2nd quartile.
- **1st Quartile** : The 25th percentile.
- **3rd Quartile** : The 75th percentile.
- **Minimum** : The smallest observation in the sample.
- **Maximum** : The largest observation in the sample.

**A quartile is an observed value at a point that aids in splitting the ordered data sample into four equally sized parts.** The median, or 2nd Quartile, splits the ordered data sample into two parts, and the 1st and 3rd quartiles split each of those halves into quarters.

**A percentile is an observed value at a point that aids in splitting the ordered data sample into 100 equally sized portions.** Quartiles are often also expressed as percentiles.

Both the quartile and percentile values are examples of rank statistics that can be calculated on a data sample with any distribution. They are used to quickly summarize how much of the data in the distribution is behind or in front of a given observed value. For example, half of the observations are behind and in front of the median of a distribution.

Note that quartiles are also calculated in the box and whisker plot, a nonparametric method to graphically summarize the distribution of a data sample.

## B.2 How to Calculate the Five-Number Summary

Calculating the five-number summary involves finding the observations for each quartile as well as the minimum and maximum observed values from the data sample.

If there is no specific value in the ordered data sample for the quartile, such as if there are an even number of observations and we are trying to find the median, then we can calculate the mean of the two closest values, such as the two middle values.

We can calculate arbitrary percentile values in Python using the `percentile` NumPy function. We can use this function to calculate the 1st, 2nd (median), and 3rd quartile values. The function takes both an array of observations and a floating point value to specify the percentile to calculate in the range of 0 to 100. It can also takes a list of percentile values to calculate multiple percentiles.
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# %matplotlib inline

# Create Random Data
dist_data=np.random.weibull(a=1,size=1000)
sns.distplot(dist_data)
print(dist_data)

# Get 1st Quartil, 2nd Quartil, and 3rd Quartil

quartiles = np.percentile(dist_data, [25, 50, 75])
print(quartiles)

"""---
The NumPy functions `min()` and `max()` can be used to return the smallest and largest values in the data sample.
"""

# Get Smallest and Largest Values

data_min, data_max = dist_data.min(), dist_data.max()

print(data_min)
print(data_max)

"""---
We can put all of this together.

The example below generates a data sample drawn from a uniform distribution between 0 and 1 and summarizes it using the five-number summary.
"""

# Calculate a 5-number summary

## Import Libraries
from numpy import percentile
from numpy.random import rand

## Generate data sample
data = rand(1000)

## Calculate quartiles
quartiles = percentile(data, [25, 50, 75])

## Calculate min/max
data_min, data_max = data.min(), data.max()

## Print 5-number summary
print('Min: %.3f' % data_min)
print('Q1: %.3f' % quartiles[0])
print('Median: %.3f' % quartiles[1])
print('Q3: %.3f' % quartiles[2])
print('Max: %.3f' % data_max)

# Display Data

print(data)
sns.distplot(data)

"""# C. Outlier

## C.1 Tukey's rule for Outlier Detection

Another famous idea by Tukey is how to use his 5-number summary for detecting outlier. The gist is that data that deviates from the median above certain threshold will be flagged as outlier. This formula is only works if the distribution is not normal.

The formula for Tukey outlier detection

${q_{3}+1.5*IQR}$

${q_{1}-1.5*IQR}$

<img src='https://miro.medium.com/max/1838/1*2c21SkzJMf3frPXPAR_gZA.png'>

---

If the distribution is normal, then we use a number called Z Score. 

<img src='https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module6-RandomError/Normal%20Distribution%20deviations.png'>

In Normal Distribution, outliers have Z Score more than 3 or less than -3.

---
Lets check our previous case.

| Name     | Grade |
|----------|-------|
| Dan      | 50    |
| Joann    | 50    |
| Pedro    | 46    |
| Rosie    | 95    |
| Ethan    | 50    |
| Vicky    | 5     |
| Frederic | 57    |
"""

# Commented out IPython magic to ensure Python compatibility.
# Visualization of Box Plot of Grade

# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats

df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],
                   'Salary':[50000,54000,50000,189000,55000,40000,59000],
                   'Hours':[41,40,36,17,35,39,40],
                   'Grade':[50,50,46,95,50,5,57]})

# Plot a box-whisker chart
df['Grade'].plot(kind='box', title='Grade Distribution', figsize=(10,8))
plt.show()

# Visualization of Distribution
grade = df['Grade']
density = stats.gaussian_kde(grade)
n, x, _ = plt.hist(grade, histtype='step', bins=25)  
plt.plot(x, density(x)*2**7.5)
plt.axvline(grade.mean(), color='magenta', linestyle='dashed', linewidth=2)
plt.axvline(grade.median(), color='green', linestyle='dashed', linewidth=2)
plt.show()

"""From above graph, we can see that based on Tukey's rule, this data have outliers. But, if we check the distribution, we also see that this data has Normal Distribution. Therefore, to determine outliers we will use Z Score instead of Tukey's rule."""

# Get Z-Score

import scipy.stats as stats

df['Z-Score'] = stats.zscore(df['Grade'])
df

"""Based on Z-Score, we can see that this data has no outliers.

---

# D. Relationship between Variables

## D.1 Covariance & Correlation

Covariance is a measure of the joint variability of two random variables. Correlation is a statistic that measures the degree to which two variables move in relation to each other.

**Covariance can be - ∞ to ∞ meanwhile Correlation only between -1 to 1. Correlation can be seen as scaled version of Covariance**

Let's take a look of this code below. We will generate `x` data and 3 types of `y` and we will see the correlation value between `x` and each of `y`.
"""

# Generate Dataset

x = np.arange(0,10,0.1)
y1 = x*0.5 + np.random.normal(scale=0.5,size=len(x))
y2 = -x*0.5 + np.random.normal(scale=0.5,size=len(x))
y3 = x*1e-3 + np.random.normal(scale=0.5,size=len(x))

# Visualization of x and y

fig,ax = plt.subplots(1,3,figsize=(12,3))
ax[0].scatter(x,y1)
ax[1].scatter(x,y2)
ax[2].scatter(x,y3)

# Get Covariance

X = np.stack((x, y1, y2, y3), axis=0)
np.cov(X)

"""This is interpretation of above result regarding of covariance between `x` and `y1`.

|    | x | y1 | y2 | y3 |
|---|---|---|---|---|
| x  | 8.41666667 | 4.25007434 | -4.22779339 | 0.13029859 |
| y1 | 4.25007434 | 2.37231351 | -2.02100975 | 0.09123685 |
| y2 | -4.22779339 | -2.02100975 | 2.34325739 | -0.08498708 |
| y3 | 0.13029859 | 0.09123685 | -0.08498708 | 0.29428203 |


"""

# Get Correlation
print('\nCorrelation between x and y1 : \n', np.corrcoef(x, y1))
print('\nCorrelation between x and y2 : \n', np.corrcoef(x, y2))
print('\nCorrelation between x and y3 : \n', np.corrcoef(x, y3))

"""This is interpretation of above result regarding of correlation between `x` and `y1`.

|    | x | y1 |
|---|---|---|
| x  | 1 | 0.95113119 |
| y1 | 0.95113119 | 1 |

|    | x | y2 |
|---|---|---|
| x  | 1 | -0.95199287 |
| y2 | -0.95199287 | 1 |

|    | x | y3 |
|---|---|---|
| x  | 1 | 0.08279189 |
| y3 | 0.08279189 | 1 |

---

# E. Usage on Real Dataset

Let's apply what we have learned so far into a real case.

## Load Dataset

Let's use dataset from [Kaggle.com](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). 

This dataset contains attributes of a house and its price. Our job is to make a prediction model that can predict price of a house based on given attributes. Each data contains more than 70 attributes/features. List of example features : 

* `SalePrice` : the property's sale price in dollars. This is the target variable that you're trying to predict.
* `MSSubClass` : The building class
* `MSZoning` : The general zoning classification
* `LotFrontage` : Linear feet of street connected to property
* `LotArea` : Lot size in square feet
* `Street` : Type of road access
* `Alley` : Type of alley access
* `LotShape` : General shape of property
* `LandContour` : Flatness of the property
* `Utilities` : Type of utilities available
* `LotConfig` : Lot configuration
* `LandSlope`: Slope of property

etc

You can find more details about the dataset from the above link.
"""

# Import Libraries

import pandas as pd
import numpy as np
import sklearn
import seaborn as sns

# Load Dataset

df_train = pd.read_csv('train.csv')
df_train.head()

# Get All Columns

df_train.columns

# Get Type of All Columns
df_train.info()

"""## E.1 Exploratory Data Analysis (EDA)

For the demonstration only, we will using ony numeric features/columns from our dataset.
"""

# For the sake of simplicity, let's just use features/columns that are numeric (int or float)

## Select only numeric features
df_train_num = df_train.select_dtypes(include= np.number)

## Save the name of numeric features into a list
columns_num = df_train_num.columns.to_list()

## Let's check it now
df_train_num.info()

# Check If Dataset Contains Missing Values

for col in columns_num:
  total_missing = df_train[col].isnull().sum()
  if total_missing > 0:
    print(col, ' : ', total_missing)

"""---
Oh no ! It seems there are features that have missing values such as `LotFrontage`, `MasVnrArea`, `GarageYrBlt`. Let's fix it !
"""

# Feature : LotFrontage

print('Skewness value : ', df_train['LotFrontage'].skew())
df_train['LotFrontage'].hist(bins=50, figsize=(10,5), color='maroon')
plt.show()

# Feature : MasVnrArea

print('Skewness value : ', df_train['MasVnrArea'].skew())
df_train['MasVnrArea'].hist(bins=50, figsize=(10,5), color='maroon')
plt.show()

# Feature : GarageYrBlt

print('Skewness value : ', df_train['GarageYrBlt'].skew())
df_train['GarageYrBlt'].hist(bins=50, figsize=(10,5), color='maroon')
plt.show()

"""---
We can see from above graph that :
* `LotFrontage` : right-skewed
* `MasVnrArea` : right-skewed
* `GarageYrBlt` : left-skewed

Because these 3 features is skewed, we will use the median value to fill missing value.
"""

# Let's See Our Dataset Information Before Handling Missing Values
df_train.describe()

# Handling Missing Values using Median

df_train['LotFrontage'] = df_train['LotFrontage'].fillna(df_train['LotFrontage'].median())
df_train['MasVnrArea'] = df_train['MasVnrArea'].fillna(df_train['MasVnrArea'].median())
df_train['GarageYrBlt'] = df_train['GarageYrBlt'].fillna(df_train['GarageYrBlt'].median())
df_train.describe()

# Check If Dataset Contains Missing Values

for col in columns_num:
  total_missing = df_train[col].isnull().sum()
  print(col, ' : ', total_missing)

print('Total Columns : ', len(columns_num))

"""---
## E.2 Feature Engineering

After handling missing value, our next job is choose features as input to our model. For this, we can use correlation.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Heatmap Correlation Matrix
# 
# f, ax = plt.subplots(figsize=(15, 10))
# sns.heatmap(df_train.corr(), vmax=.8, square=True);
# 
# plt.figure(figsize=(15,10))
# sns.heatmap(df_train.corr(), annot=True, fmt='.2f')
# plt.show()
#

"""---

Select `n` features as our input features.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Heatmap with Only Selected Numbers
# k = 18 # number of variables for heatmap
# plt.figure(figsize=(15,10))
# cols = df_train.corr().nlargest(k, 'SalePrice')['SalePrice'].index
# cm = np.corrcoef(df_train[cols].values.T)
# sns.set(font_scale=1.25)
# hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
# plt.show()

# Get List of All Correlation of SalePrice

df_train.corr().nlargest(38, 'SalePrice')['SalePrice']

"""---
We also can set a threshold as lower limit number for correlation value. Let's say we set the threshold is `0.3`. It means, that our input features only features that has correlation with `SalePrice` with value higher or equal `0.3`. 
"""

# Get Features that Has Correlation Higher or Equal than Threshold

# Check if correlation value is higher or equal than 0.3
columns_check = df_train.corr()['SalePrice'] >= 0.3
print('Columns Check\n')
print(columns_check)
print('')

# Get all columns that has correlation value higher or equal tahn 0.3
columns_final = columns_check[columns_check==True].index
print('Columns Final       : ', columns_final)
print('Total Columns Final : ', len(columns_final))