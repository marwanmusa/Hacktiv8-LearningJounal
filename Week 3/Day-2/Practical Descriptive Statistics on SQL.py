# -*- coding: utf-8 -*-
"""d2pm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ardhiraka/FSDS_Guidelines/blob/master/p0/w3/d2pm.ipynb

# Day 2 PM - Practical Descriptive Statistics on SQL

You have already learnt basic SQL query syntaxes including DML, DDL, and DQL, especially today, we learn about group by, rank, and row_number. In this notebook, we will use our SQL knowledge to solve descriptive statistics problems. To narrowing our learning scope, we only use sqlite but in general, the concept and the syntax are not different so far. Furthermore, you can use other IDE such as Dbeaver, Datagrip, Navicat, etc.
"""

import sqlite3
import pandas as pd

"""## The Dataset
For this learning, we use data from Our World in Data about air pollution. You can access the github to look at this data or others: https://github.com/owid/owid-datasets/tree/master/datasets . You don't need download any data, we access the link directly using Pandas.

We have two csv files which are air pollutant emission from OECD and air pollutant death breakdown by age from IMHE. The description of each data delivered below:

1. **Air pollutant emission by OECD**\
Air pollutant emissions reported for OECD countries, where data is available, measured in tonnes per year.\
Indexed figures relate to changes since the year 1990 (1990 is assumed equal to 100). A figure lower than 100 indicates a reduction in emissions (e.g. 40 indicates a 60% reduction since 1990). Indexed figures are only available for countries with data extending to 1990.


2. **Air pollutant death breakdown by IMHE**\
Data relates to the number of deaths attributed to air pollution across age groups. This is measured based on attribution to all linked causes.\
IHME have developed a Comparative Risk Assessment (CRA) conceptual framework by which they have built a web of risk factors or causes which affect health outcomes.\
Such risk-outcome pairs (e.g. air pollution and lung cancer) are formed based on evidence links using methods such as cohort studies, randomised trials, and case-control studies. Once a risk-outcome pair has been formed, how does IHME begin to quantify the disease burden or number of deaths attributed to each risk? The CRA can be used for two different types of assessment: attributable burden and avoidable burden. 'Attributable burden' represents the reduction in current disease burden (or that of a given year) if population exposure had shifted to another counterfactual/hypothetical exposure level; 'avoidable burden' represents the potential future avoided burden if population exposure was to shift to a counterfactual level of exposure. Since the number of deaths is based on current or historical data, the data presented here is that of attributable burden.
"""

air_pollutant_emission = pd.read_csv('https://github.com/owid/owid-datasets/raw/master/datasets/Air%20Pollutant%20Emissions%20-%20OECD/Air%20Pollutant%20Emissions%20-%20OECD.csv')
air_pol_death_by_age = pd.read_csv('https://raw.githubusercontent.com/owid/owid-datasets/7c8e4abdfde611322781da5fbc9d1b8333645370/datasets/Air%20pollution%20deaths%20breakdown%20by%20age%20-%20IHME/Air%20pollution%20deaths%20breakdown%20by%20age%20-%20IHME.csv')

air_pollutant_emission.head()

"""General overview of air pollutant emission data. We see that there are some nulls in our data. let check them out that how much the nulls are in each column."""

air_pollutant_emission.info()

"""The actual size of the data is 910 rows and seem that several columns severe the nulls. Since the data is numerical, we assume that the nulls can be imputed by 0, which mean no data. The aim of missing values handling in this hands on, we want to ease our work when we input this data into SQL table. We do not want to make our life difficult, don't we?"""

air_pollutant_emission.fillna(0,inplace=True)
air_pollutant_emission.info()

"""Hurray! Our data is cleaned from missing value, at least there are no nulls anymore."""

air_pol_death_by_age.head()

"""We can see above that our second data seems okay, we have 36 columns which is a lot of work to do later. However, we have to check the data summary whether the nulls exist or not."""

air_pol_death_by_age.info()

"""There are no nulls in our sencod data and this is a relief. Finally, we will do some SQL stuffs with our SQL database.

### Creating and Connecting the SQLite Database

First of all, since we don't have a database, let's create sqlite database file named "day2pm.sqlite" or anything else that you want. But, before we do that, we need to define a function to create a connection to our sqlite database just like below:
"""

def create_connection(path):
    connection=sqlite3.connect(path)
    return(connection)

connection = create_connection("day2pm.sqlite")

"""After we create the database and connection, we need to define a function to send our query into the database. This function used for table creation and data insertion process. """

def execute_query(connection, query):
    cursor = connection.cursor()
    cursor.execute(query)
    connection.commit()

"""### Creating Tables

Next, we define the DDL query to create the tables. We have two data so we have to make two tables which are air_pollutant_emission and air_pollutant_death_by_age in seperate query. Please adjust the column name with the pandas dataframes columns. You can edit the columns name as comfortable as you. Be carefull of the data type, check the data before you set the data type on the queries. After defining the queries for each table, we call the execute function and input the connection and query as the arguments for creating the tables.
"""

# Create air_pollutant_emission table query

create_table_1 = ''' 
CREATE TABLE IF NOT EXISTS air_pollutant_emission (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    Entity TEXT NOT NULL,
    Year INTEGER,
    CO FLOAT,
    NOx FLOAT,
    Non_Methane_VOCs FLOAT,
    PM1_0 FLOAT,
    PM2_5 FLOAT,
    SO2 FLOAT,
    CO_index FLOAT,
    NOx_index FLOAT,
    Non_Methane_VOCs_index FLOAT,
    PM1_0_index FLOAT,
    PM2_5_index FLOAT,
    SO2_index FLOAT
)
'''

# Create air_pollutant_death_by_age table query

create_table_2 = '''
CREATE TABLE IF NOT EXISTS air_pollutant_death_by_age (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    Entity TEXT NOT NULL,
    Year INTEGER,
    Indoor_Under_5 INTEGER,
    Indoor_5_to_9 INTEGER,
    Indoor_10_to_14 INTEGER,
    Indoor_15_to_19 INTEGER,
    Indoor_20_to_24 INTEGER,
    Indoor_25_to_29 INTEGER,
    Indoor_30_to_34 INTEGER,
    Indoor_35_to_39 INTEGER,
    Indoor_40_to_44 INTEGER,
    Indoor_45_to_49 INTEGER,
    Indoor_50_to_54 INTEGER,
    Indoor_55_to_59 INTEGER,
    Indoor_60_to_64 INTEGER,
    Indoor_65_to_69 INTEGER,
    Indoor_70_to_74 INTEGER,
    Indoor_75_to_79 INTEGER,
    Indoor_80_plus INTEGER,
    Outdoor_Under_5 INTEGER,
    Outdoor_5_to_9 INTEGER,
    Outdoor_10_to_14 INTEGER,
    Outdoor_15_to_19 INTEGER,
    Outdoor_20_to_24 INTEGER,
    Outdoor_25_to_29 INTEGER,
    Outdoor_30_to_34 INTEGER,
    Outdoor_35_to_39 INTEGER,
    Outdoor_40_to_44 INTEGER,
    Outdoor_45_to_49 INTEGER,
    Outdoor_50_to_54 INTEGER,
    Outdoor_55_to_59 INTEGER,
    Outdoor_60_to_64 INTEGER,
    Outdoor_65_to_69 INTEGER,
    Outdoor_70_to_74 INTEGER,
    Outdoor_75_to_79 INTEGER,
    Outdoor_80_plus INTEGER
)
'''

execute_query(connection, create_table_1)
execute_query(connection, create_table_2)

"""### Inserting the Data

In previous step, we only created the tables without inputing the data. In this step, we need to insert the data from the dataframes into the database tables. This is a lot of work actually so please be patient and careful. We use for loop to access each row on each dataframes and insert it to the data one by one.
"""

for i in range(len(air_pollutant_emission)):
    dat = air_pollutant_emission.values[i,:]
    insert_values_into_table_1 = f'''
    INSERT INTO air_pollutant_emission (
        Entity, Year, CO, NOx, Non_Methane_VOCs, PM1_0, PM2_5, SO2, CO_index, NOx_index, Non_Methane_VOCs_index, PM1_0_index, PM2_5_index, SO2_index)
    VALUES ("{dat[0]}", {dat[1]}, {dat[2]}, {dat[3]}, {dat[4]}, {dat[5]}, {dat[6]}, {dat[7]}, {dat[8]}, {dat[9]}, {dat[10]}, {dat[11]}, {dat[12]}, {dat[13]})
    '''
    execute_query(connection, insert_values_into_table_1)

for i in range(len(air_pol_death_by_age)):
    dat = air_pol_death_by_age.values[i,:]
    insert_values_into_table_2 = f'''
    INSERT INTO air_pollutant_death_by_age (
        Entity, Year, Indoor_Under_5, Indoor_5_to_9, Indoor_10_to_14, Indoor_15_to_19, Indoor_20_to_24, Indoor_25_to_29, Indoor_30_to_34, Indoor_35_to_39,
        Indoor_40_to_44, Indoor_45_to_49, Indoor_50_to_54, Indoor_55_to_59, Indoor_60_to_64, Indoor_65_to_69, Indoor_70_to_74, Indoor_75_to_79, Indoor_80_plus,
        Outdoor_Under_5, Outdoor_5_to_9, Outdoor_10_to_14, Outdoor_15_to_19, Outdoor_20_to_24, Outdoor_25_to_29, Outdoor_30_to_34, Outdoor_35_to_39,
        Outdoor_40_to_44, Outdoor_45_to_49, Outdoor_50_to_54, Outdoor_55_to_59, Outdoor_60_to_64, Outdoor_65_to_69, Outdoor_70_to_74, Outdoor_75_to_79, Outdoor_80_plus)
    
    VALUES ("{dat[0]}", {dat[1]}, {dat[18]}, {dat[10]}, {dat[2]}, {dat[3]}, {dat[4]}, {dat[5]}, {dat[6]}, {dat[7]}, {dat[8]}, {dat[9]}, {dat[11]},
            {dat[12]}, {dat[13]}, {dat[14]}, {dat[15]}, {dat[16]}, {dat[17]}, {dat[35]}, {dat[27]}, {dat[19]}, {dat[20]}, {dat[21]}, {dat[22]},
            {dat[23]}, {dat[24]}, {dat[25]}, {dat[26]}, {dat[28]}, {dat[29]}, {dat[30]}, {dat[31]}, {dat[32]}, {dat[33]}, {dat[34]})
    '''
    execute_query(connection, insert_values_into_table_2)

"""Yeay, we have already transfer our dataframes into sqlite database successfully. To check our data or tables, please run the cell below:"""

cursor = connection.cursor()

cursor.execute("SELECT name FROM sqlite_sequence;")
print(cursor.fetchall())

"""Our tables are successfully created and now we check the data of each table. We will ask Pandas for help."""

pd.read_sql_query('select * from air_pollutant_emission limit 5',connection)

pd.read_sql_query('select * from air_pollutant_death_by_age limit 5',connection)

"""Bravo! We did great.

## Practical Descriptive Statistics Problems

Minister of Environment and Foresty of The Republic of Indonesia, Mrs. Siti Nurbaya Bakar assigns a Data Scientist in her ministry which is YOU to give her some insights about the air pollution condition in OECD countries. To make your works easier, She has some questions that you have to based on data that stored in the sqlite database. The questions are:

1. Which OECD country that has the 16th-lowest in 1990 on CO level? Could you provide the data?
2. If we compare to the question number 1, which OECD country that has the 16th-lowest in 2005? is the answer still the same? You don't need give the data, just the country name. Give me a name.
3. Please give me a table contains the SO2 level in 1990 and the last of year record of Poland. Did it increase or decrease?
4. Which OECD country(s) that the PM1.0 level is in maximum value before 2000?
5. Could you please provide the death breakdown data by age of the top 2 of highest NOx level of OECD countries for Indoor Under 5 by mean, max, and min?

### Question 1

**Which OECD country that has the 16th-lowest in 1990 on CO level? Could you provide the data?**
"""

q1_a = '''
select Entity, Year, CO,
        rank() over(
            order by CO asc
           ) as rank
from air_pollutant_emission
where Entity != 'OECD - Total' and Entity != 'Europe' and Year=1990
group by Entity, Year, CO;
'''
pd.read_sql_query(q1_a,connection)

"""We can see that if we use rank() and use order by CO, Greece and The Netherlands have the same rank since they have the same CO level in 1990. So, the answer is both Greece and The Netherlands. Different with rank(), row_number() will give ordered number based on the order and it does not care about the value. So the result is:"""

q1_b = '''
select Entity, Year, CO,
        row_number() over(
            order by CO asc
           ) as rank
       from air_pollutant_emission
where Entity != 'OECD - Total' and Entity != 'Europe' and Year=1990
group by Entity, Year, CO;
'''
pd.read_sql_query(q1_b,connection)

"""If we asked that which country that has the 16th-lowest CO level in 2016, we only have an answer which is Greece based on row_number() calculation.

### Question 2

**If we compare to the question number 1, which OECD country that has the 16th-lowest in 2005? is the answer still the same? You don't need give the data, just the country name. Give me a name.**

Since Mrs. Minister want a name only, we use row_number instead and we do not need Pandas since we do not show the table
"""

q2 = '''
select Entity from (

    select Entity,
        row_number() over(
            order by CO asc
           ) as rank
    from air_pollutant_emission as ape
    where Entity != 'OECD - Total' and Entity != 'Europe' and Year=2005
    group by Entity, Year, CO)

where rank=16
;
'''
cursor = connection.cursor()
cursor.execute(q2)
print('The 16th-lowest CO level in 2005:',cursor.fetchall()[0][0])

"""Greece is not longer be the 16th-lowest CO Level in 2005, but Czech Republic instead.

### Question 3

**Please give me a table contains the SO2 level in 1990 and the last of year record of Poland. Did it increase or decrease?**
"""

q3 = '''
select Entity, Year, SO2 from air_pollutant_emission as ape
where Entity = 'Poland' and (Year = 1990 or Year = (select max(Year) from air_pollutant_emission where Entity='Poland' ))
group by Entity, SO2
order by Year
'''
pd.read_sql_query(q3,connection)

"""In 2015, SO2 emission level of Poland is decreased from 1990

### Question 4

**Which OECD country(s) that the PM1.0 level is in maximum value before 2000?**
"""

q4='''
select * from (select Entity, Year, max(PM1_0) as "Max PM1.0" from air_pollutant_emission as ape
where Entity!="Europe" and Entity!="OECD - Total" group by Entity)
where Year < 2000
'''
pd.read_sql_query(q4,connection)

"""### Question 5

**Could you please provide the death breakdown data by age of the top 2 of highest NOx level of OECD countries in 2015 for Indoor Under 5 by mean, maximum, and minimum?**
"""

q5 = '''
select dat.Entity, avg(apda.Indoor_Under_5) as mean,
       max(apda.Indoor_Under_5) as maximum,
       min(apda.Indoor_Under_5) as minimum
from (select Entity,NOx,rank() over(order by NOx desc) as rank
        from air_pollutant_emission
        where Entity!="Europe" and Entity!="OECD - Total" and Year=2015
        group by Entity, NOx limit 2) as dat
join air_pollutant_death_by_age as apda on dat.Entity = apda.Entity
group by dat.Entity;
'''

pd.read_sql_query(q5,connection)